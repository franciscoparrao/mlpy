name: Benchmarks

on:
  push:
    branches: [ main ]
    paths:
      - 'mlpy/**'
      - 'benchmarks/**'
  pull_request:
    branches: [ main ]
    paths:
      - 'mlpy/**'
      - 'benchmarks/**'
  schedule:
    # Run benchmarks weekly on Sundays at 2 AM UTC
    - cron: '0 2 * * 0'
  workflow_dispatch:

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - uses: actions/checkout@v5
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: "3.11"
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-benchmark-${{ hashFiles('**/requirements*.txt') }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e .
        pip install pytest-benchmark
        pip install matplotlib seaborn plotly
        pip install scikit-learn torch torchvision --index-url https://download.pytorch.org/whl/cpu
        # Install TGPY if available
        if [ -d "tgpy-master" ]; then
          pip install -e tgpy-master/
        fi
    
    - name: Run quick benchmarks
      run: |
        python -c "
        import sys
        import numpy as np
        from mlpy.benchmark import Benchmark
        from mlpy.tasks import TaskRegr
        from mlpy.learners.sklearn import LearnerRegrLM, LearnerRegrRF
        from mlpy.resamplings import ResamplingCV
        from mlpy.measures import MeasureRegrRMSE
        import pandas as pd
        
        print('Running quick MLPY benchmark...')
        
        # Create simple synthetic data
        np.random.seed(42)
        X = np.random.randn(100, 3)
        y = X.sum(axis=1) + 0.1 * np.random.randn(100)
        data = pd.DataFrame(X, columns=['x1', 'x2', 'x3'])
        data['y'] = y
        
        # Create task
        task = TaskRegr(id='synthetic', data=data, target='y')
        
        # Create learners
        learners = [
            LearnerRegrLM(id='lm'),
            LearnerRegrRF(id='rf', n_estimators=10)  # Small RF for speed
        ]
        
        # Create benchmark
        benchmark = Benchmark(
            tasks=[task],
            learners=learners,
            resamplings=[ResamplingCV(id='cv3', folds=3)],  # Quick CV
            measures=[MeasureRegrRMSE(id='rmse')]
        )
        
        # Run benchmark
        results = benchmark.run()
        print('Benchmark completed successfully!')
        print(f'Results shape: {results.performance.shape}')
        print(results.performance.head())
        "
    
    - name: Run performance benchmarks
      run: |
        python -c "
        import time
        import numpy as np
        from mlpy.learners.sklearn import LearnerRegrLM, LearnerRegrRF
        from mlpy.tasks import TaskRegr
        import pandas as pd
        
        print('Running performance tests...')
        
        # Test scaling with different data sizes
        sizes = [100, 500, 1000]
        results = []
        
        for n in sizes:
            print(f'Testing with {n} samples...')
            
            # Generate data
            np.random.seed(42)
            X = np.random.randn(n, 5)
            y = X.sum(axis=1) + 0.1 * np.random.randn(n)
            data = pd.DataFrame(X, columns=[f'x{i}' for i in range(5)])
            data['y'] = y
            
            task = TaskRegr(id=f'test_{n}', data=data, target='y')
            learner = LearnerRegrLM(id='lm')
            
            # Time training
            start = time.time()
            learner.train(task)
            train_time = time.time() - start
            
            # Time prediction
            start = time.time()
            pred = learner.predict(task)
            pred_time = time.time() - start
            
            results.append({
                'n_samples': n,
                'train_time': train_time,
                'pred_time': pred_time
            })
            
            print(f'  Train time: {train_time:.4f}s, Pred time: {pred_time:.4f}s')
        
        print('Performance test completed!')
        "
    
    - name: Create benchmark report
      run: |
        echo '# Benchmark Report' > benchmark_report.md
        echo '' >> benchmark_report.md
        echo '## System Information' >> benchmark_report.md
        echo '- OS: Ubuntu Latest' >> benchmark_report.md
        echo '- Python: 3.11' >> benchmark_report.md
        echo "- Timestamp: $(date)" >> benchmark_report.md
        echo '' >> benchmark_report.md
        echo '## Test Results' >> benchmark_report.md
        echo 'All basic benchmarks passed successfully.' >> benchmark_report.md
        echo '' >> benchmark_report.md
        echo '## Performance Notes' >> benchmark_report.md
        echo '- MLPY components are functioning correctly' >> benchmark_report.md
        echo '- sklearn integration working as expected' >> benchmark_report.md
        echo '- TGPY integration available (if installed)' >> benchmark_report.md
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-report
        path: benchmark_report.md
    
    - name: Comment PR with benchmark results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          const report = fs.readFileSync('benchmark_report.md', 'utf8');
          
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: `## ðŸš€ Benchmark Results\n\n${report}`
          });
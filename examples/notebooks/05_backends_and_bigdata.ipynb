{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLPY: Backends y Big Data\n",
    "\n",
    "Este notebook demuestra las capacidades de MLPY para trabajar con diferentes backends de datos:\n",
    "- **Pandas**: Para datos medianos (< 1M filas)\n",
    "- **NumPy**: Para datos num√©ricos puros\n",
    "- **Dask**: Para big data distribuido\n",
    "- **Vaex**: Para datasets masivos (>1B filas)\n",
    "- **Backend combinations**: Cbind y Rbind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importar backends de MLPY\n",
    "from mlpy.backends import (\n",
    "    DataBackendPandas, \n",
    "    DataBackendNumPy,\n",
    "    DataBackendCbind,\n",
    "    DataBackendRbind\n",
    ")\n",
    "\n",
    "# Intentar importar backends opcionales\n",
    "try:\n",
    "    import dask.dataframe as dd\n",
    "    from mlpy.backends import DataBackendDask\n",
    "    DASK_AVAILABLE = True\n",
    "    print(\"‚úÖ Dask disponible\")\n",
    "except ImportError:\n",
    "    DASK_AVAILABLE = False\n",
    "    print(\"‚ùå Dask no disponible\")\n",
    "\n",
    "try:\n",
    "    import vaex\n",
    "    from mlpy.backends import DataBackendVaex\n",
    "    VAEX_AVAILABLE = True\n",
    "    print(\"‚úÖ Vaex disponible\")\n",
    "except ImportError:\n",
    "    VAEX_AVAILABLE = False\n",
    "    print(\"‚ùå Vaex no disponible\")\n",
    "\n",
    "from mlpy.tasks import TaskClassif\n",
    "from mlpy.learners import LearnerClassifSklearn\n",
    "from mlpy.measures import MeasureClassifAccuracy\n",
    "from mlpy.resamplings import ResamplingHoldout\n",
    "from mlpy import resample\n",
    "\n",
    "print(\"\\nüöÄ Backends de MLPY listos para usar!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Backend Pandas - Dataset Est√°ndar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear dataset sint√©tico de tama√±o mediano\n",
    "np.random.seed(42)\n",
    "n_samples = 50000  # 50K filas\n",
    "n_features = 15\n",
    "\n",
    "print(f\"üìä Creando dataset con Pandas: {n_samples:,} filas x {n_features} columnas\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Generar datos\n",
    "data = np.random.randn(n_samples, n_features)\n",
    "feature_names = [f'feature_{i}' for i in range(n_features)]\n",
    "df = pd.DataFrame(data, columns=feature_names)\n",
    "\n",
    "# A√±adir algunas columnas categ√≥ricas\n",
    "df['category'] = np.random.choice(['A', 'B', 'C', 'D'], n_samples)\n",
    "df['region'] = np.random.choice(['North', 'South', 'East', 'West'], n_samples)\n",
    "\n",
    "# Target basado en algunas features\n",
    "target_score = (\n",
    "    2 * df['feature_0'] + \n",
    "    1.5 * df['feature_1'] - \n",
    "    0.8 * df['feature_2'] +\n",
    "    (df['category'] == 'A').astype(int) * 1.5 +\n",
    "    np.random.normal(0, 0.5, n_samples)\n",
    ")\n",
    "df['target'] = (target_score > target_score.median()).astype(int)\n",
    "\n",
    "creation_time = time.time() - start_time\n",
    "print(f\"‚è±Ô∏è  Dataset creado en {creation_time:.2f}s\")\n",
    "print(f\"üìã Shape: {df.shape}\")\n",
    "print(f\"üíæ Memoria: ~{df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear backend Pandas\n",
    "print(\"üîß Creando DataBackendPandas...\")\n",
    "\n",
    "start_time = time.time()\n",
    "backend_pandas = DataBackendPandas(df)\n",
    "backend_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Backend creado en {backend_time:.3f}s\")\n",
    "print(f\"\\nüìä Propiedades del backend:\")\n",
    "print(f\"   - Filas: {backend_pandas.nrow:,}\")\n",
    "print(f\"   - Columnas: {backend_pandas.ncol}\")\n",
    "print(f\"   - Hash: {backend_pandas.hash[:16]}...\")\n",
    "print(f\"   - Nombres de columnas: {backend_pandas.colnames[:5]}...\")\n",
    "\n",
    "# Operaciones b√°sicas\n",
    "print(f\"\\nüîç Operaciones b√°sicas:\")\n",
    "\n",
    "# Head\n",
    "start = time.time()\n",
    "head = backend_pandas.head(5)\n",
    "print(f\"   - Head(5): {time.time()-start:.3f}s\")\n",
    "\n",
    "# Distinct values\n",
    "start = time.time()\n",
    "distinct = backend_pandas.distinct(['category'])\n",
    "print(f\"   - Distinct(category): {time.time()-start:.3f}s -> {distinct['category']}\")\n",
    "\n",
    "# Missing values\n",
    "start = time.time()\n",
    "missing = backend_pandas.missings()\n",
    "print(f\"   - Missings(): {time.time()-start:.3f}s -> {missing} valores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backend NumPy - Datos Num√©ricos Puros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer solo datos num√©ricos para NumPy\n",
    "numeric_columns = [col for col in df.columns if col.startswith('feature_') or col == 'target']\n",
    "numeric_data = df[numeric_columns].values\n",
    "\n",
    "print(f\"üî¢ Creando DataBackendNumPy con {numeric_data.shape[0]:,} x {numeric_data.shape[1]} datos\")\n",
    "\n",
    "start_time = time.time()\n",
    "backend_numpy = DataBackendNumPy(numeric_data, colnames=numeric_columns)\n",
    "numpy_time = time.time() - start_time\n",
    "\n",
    "print(f\"‚úÖ Backend NumPy creado en {numpy_time:.3f}s\")\n",
    "print(f\"\\nüìä Propiedades NumPy:\")\n",
    "print(f\"   - Filas: {backend_numpy.nrow:,}\")\n",
    "print(f\"   - Columnas: {backend_numpy.ncol}\")\n",
    "print(f\"   - Tipo de datos: {backend_numpy._data.dtype}\")\n",
    "print(f\"   - Memoria: ~{backend_numpy._data.nbytes / 1024**2:.1f} MB\")\n",
    "\n",
    "# Comparar operaciones NumPy vs Pandas\n",
    "print(f\"\\n‚ö° Comparaci√≥n de rendimiento:\")\n",
    "\n",
    "# Head operation\n",
    "start = time.time()\n",
    "_ = backend_pandas.head(1000)\n",
    "pandas_head_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "_ = backend_numpy.head(1000)\n",
    "numpy_head_time = time.time() - start\n",
    "\n",
    "print(f\"   - Head(1000): Pandas {pandas_head_time:.4f}s vs NumPy {numpy_head_time:.4f}s\")\n",
    "print(f\"     Speedup: {pandas_head_time/numpy_head_time:.1f}x\")\n",
    "\n",
    "# Data access\n",
    "start = time.time()\n",
    "_ = backend_pandas.data(rows=list(range(10000)))\n",
    "pandas_data_time = time.time() - start\n",
    "\n",
    "start = time.time()\n",
    "_ = backend_numpy.data(rows=list(range(10000)))\n",
    "numpy_data_time = time.time() - start\n",
    "\n",
    "print(f\"   - Data(10k rows): Pandas {pandas_data_time:.4f}s vs NumPy {numpy_data_time:.4f}s\")\n",
    "print(f\"     Speedup: {pandas_data_time/numpy_data_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backends Combinados: Cbind y Rbind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear m√∫ltiples backends para combinar\n",
    "print(\"üîó Demostrando backends combinados (Cbind y Rbind)\")\n",
    "\n",
    "# Dividir datos para Cbind (columnas)\n",
    "df1 = df[['feature_0', 'feature_1', 'feature_2']].copy()\n",
    "df2 = df[['feature_3', 'feature_4', 'category']].copy()\n",
    "df3 = df[['target']].copy()\n",
    "\n",
    "backend1 = DataBackendPandas(df1)\n",
    "backend2 = DataBackendPandas(df2)\n",
    "backend3 = DataBackendPandas(df3)\n",
    "\n",
    "print(f\"\\nüìÇ Backends individuales:\")\n",
    "print(f\"   - Backend 1: {backend1.nrow:,} x {backend1.ncol} ({backend1.colnames})\")\n",
    "print(f\"   - Backend 2: {backend2.nrow:,} x {backend2.ncol} ({backend2.colnames})\")\n",
    "print(f\"   - Backend 3: {backend3.nrow:,} x {backend3.ncol} ({backend3.colnames})\")\n",
    "\n",
    "# Combinar con Cbind (column bind)\n",
    "start_time = time.time()\n",
    "backend_cbind = DataBackendCbind([backend1, backend2, backend3])\n",
    "cbind_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüîó DataBackendCbind creado en {cbind_time:.3f}s\")\n",
    "print(f\"   - Dimensiones: {backend_cbind.nrow:,} x {backend_cbind.ncol}\")\n",
    "print(f\"   - Columnas combinadas: {backend_cbind.colnames}\")\n",
    "\n",
    "# Verificar que funciona\n",
    "sample_cbind = backend_cbind.data(rows=list(range(5)))\n",
    "print(f\"\\n‚úÖ Muestra de datos combinados:\")\n",
    "print(sample_cbind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrar Rbind (row bind)\n",
    "print(\"\\nüìö Demostrando DataBackendRbind (combinaci√≥n de filas)\")\n",
    "\n",
    "# Dividir dataset en pedazos por filas\n",
    "chunk_size = len(df) // 3\n",
    "df_chunk1 = df.iloc[:chunk_size].copy()\n",
    "df_chunk2 = df.iloc[chunk_size:2*chunk_size].copy()\n",
    "df_chunk3 = df.iloc[2*chunk_size:].copy()\n",
    "\n",
    "chunk_backend1 = DataBackendPandas(df_chunk1)\n",
    "chunk_backend2 = DataBackendPandas(df_chunk2)\n",
    "chunk_backend3 = DataBackendPandas(df_chunk3)\n",
    "\n",
    "print(f\"üìä Chunks individuales:\")\n",
    "print(f\"   - Chunk 1: {chunk_backend1.nrow:,} x {chunk_backend1.ncol}\")\n",
    "print(f\"   - Chunk 2: {chunk_backend2.nrow:,} x {chunk_backend2.ncol}\")\n",
    "print(f\"   - Chunk 3: {chunk_backend3.nrow:,} x {chunk_backend3.ncol}\")\n",
    "\n",
    "# Combinar con Rbind\n",
    "start_time = time.time()\n",
    "backend_rbind = DataBackendRbind([chunk_backend1, chunk_backend2, chunk_backend3])\n",
    "rbind_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nüîó DataBackendRbind creado en {rbind_time:.3f}s\")\n",
    "print(f\"   - Dimensiones finales: {backend_rbind.nrow:,} x {backend_rbind.ncol}\")\n",
    "print(f\"   - Filas originales: {len(df):,}\")\n",
    "print(f\"   - ‚úÖ Coincide: {backend_rbind.nrow == len(df)}\")\n",
    "\n",
    "# Verificar integridad\n",
    "sample_rbind = backend_rbind.data(rows=[0, chunk_size, 2*chunk_size])\n",
    "original_sample = df.iloc[[0, chunk_size, 2*chunk_size]]\n",
    "\n",
    "print(f\"\\nüîç Verificaci√≥n de integridad (primeras columnas):\")\n",
    "print(\"Rbind:\")\n",
    "print(sample_rbind[['feature_0', 'feature_1', 'target']].to_string())\n",
    "print(\"\\nOriginal:\")\n",
    "print(original_sample[['feature_0', 'feature_1', 'target']].to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Backend Dask (si est√° disponible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DASK_AVAILABLE:\n",
    "    print(\"üöÄ Probando DataBackendDask para Big Data\")\n",
    "    \n",
    "    # Crear un dataset m√°s grande para Dask\n",
    "    print(\"üìä Creando dataset grande para Dask...\")\n",
    "    \n",
    "    # Crear Dask DataFrame\n",
    "    start_time = time.time()\n",
    "    dask_df = dd.from_pandas(df, npartitions=8)\n",
    "    dask_creation_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Dask DataFrame creado en {dask_creation_time:.3f}s\")\n",
    "    print(f\"   - Particiones: {dask_df.npartitions}\")\n",
    "    print(f\"   - Columnas: {len(dask_df.columns)}\")\n",
    "    \n",
    "    # Crear backend Dask\n",
    "    start_time = time.time()\n",
    "    backend_dask = DataBackendDask(dask_df)\n",
    "    backend_creation_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"üîß DataBackendDask creado en {backend_creation_time:.3f}s\")\n",
    "    print(f\"\\nüìä Propiedades Dask:\")\n",
    "    print(f\"   - Filas: {backend_dask.nrow:,}\")\n",
    "    print(f\"   - Columnas: {backend_dask.ncol}\")\n",
    "    print(f\"   - Particiones: {dask_df.npartitions}\")\n",
    "    \n",
    "    # Operaciones con Dask\n",
    "    print(f\"\\n‚ö° Operaciones con Dask:\")\n",
    "    \n",
    "    # Head (deber√≠a ser r√°pido)\n",
    "    start = time.time()\n",
    "    dask_head = backend_dask.head(5)\n",
    "    dask_head_time = time.time() - start\n",
    "    print(f\"   - Head(5): {dask_head_time:.3f}s\")\n",
    "    \n",
    "    # Distinct (requiere computaci√≥n)\n",
    "    start = time.time()\n",
    "    dask_distinct = backend_dask.distinct(['category'])\n",
    "    dask_distinct_time = time.time() - start\n",
    "    print(f\"   - Distinct(category): {dask_distinct_time:.3f}s -> {dask_distinct['category']}\")\n",
    "    \n",
    "    # Missing values\n",
    "    start = time.time()\n",
    "    dask_missing = backend_dask.missings()\n",
    "    dask_missing_time = time.time() - start\n",
    "    print(f\"   - Missings(): {dask_missing_time:.3f}s -> {dask_missing} valores\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Comparaci√≥n Pandas vs Dask:\")\n",
    "    print(f\"   - Creation: Pandas {backend_time:.3f}s vs Dask {backend_creation_time:.3f}s\")\n",
    "    print(f\"   - Head: Pandas {pandas_head_time:.4f}s vs Dask {dask_head_time:.4f}s\")\n",
    "\nelse:\n",
    "    print(\"‚ùå Dask no est√° disponible. Para instalarlo:\")\n",
    "    print(\"   pip install dask[dataframe]\")\n",
    "    print(\"\\nDask es ideal para:\")\n",
    "    print(\"   - Datasets que no caben en memoria\")\n",
    "    print(\"   - Procesamiento paralelo\")\n",
    "    print(\"   - Computaci√≥n distribuida\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backend Vaex (si est√° disponible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VAEX_AVAILABLE:\n",
    "    print(\"üåü Probando DataBackendVaex para Datasets Masivos\")\n",
    "    \n",
    "    # Crear Vaex DataFrame\n",
    "    start_time = time.time()\n",
    "    # Solo usar columnas num√©ricas para Vaex (m√°s eficiente)\n",
    "    numeric_df = df.select_dtypes(include=[np.number])\n",
    "    vaex_df = vaex.from_pandas(numeric_df)\n",
    "    vaex_creation_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Vaex DataFrame creado en {vaex_creation_time:.3f}s\")\n",
    "    print(f\"   - Filas: {len(vaex_df):,}\")\n",
    "    print(f\"   - Columnas: {len(vaex_df.columns)}\")\n",
    "    print(f\"   - Memoria virtual: ~{vaex_df.nbytes/1024**2:.1f} MB\")\n",
    "    \n",
    "    # Crear backend Vaex\n",
    "    start_time = time.time()\n",
    "    backend_vaex = DataBackendVaex(vaex_df)\n",
    "    vaex_backend_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"üîß DataBackendVaex creado en {vaex_backend_time:.3f}s\")\n",
    "    print(f\"\\nüìä Propiedades Vaex:\")\n",
    "    print(f\"   - Filas: {backend_vaex.nrow:,}\")\n",
    "    print(f\"   - Columnas: {backend_vaex.ncol}\")\n",
    "    \n",
    "    # Operaciones con Vaex (deber√≠an ser muy r√°pidas)\n",
    "    print(f\"\\n‚ö° Operaciones con Vaex:\")\n",
    "    \n",
    "    # Head\n",
    "    start = time.time()\n",
    "    vaex_head = backend_vaex.head(10)\n",
    "    vaex_head_time = time.time() - start\n",
    "    print(f\"   - Head(10): {vaex_head_time:.4f}s\")\n",
    "    \n",
    "    # Missing values\n",
    "    start = time.time()\n",
    "    vaex_missing = backend_vaex.missings()\n",
    "    vaex_missing_time = time.time() - start\n",
    "    print(f\"   - Missings(): {vaex_missing_time:.4f}s -> {vaex_missing} valores\")\n",
    "    \n",
    "    # Data access\n",
    "    start = time.time()\n",
    "    vaex_sample = backend_vaex.data(rows=list(range(1000)))\n",
    "    vaex_data_time = time.time() - start\n",
    "    print(f\"   - Data(1k rows): {vaex_data_time:.4f}s\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Comparaci√≥n de rendimiento:\")\n",
    "    print(f\"   - Creation: Pandas {backend_time:.3f}s vs Vaex {vaex_backend_time:.3f}s\")\n",
    "    print(f\"   - Head: Pandas {pandas_head_time:.4f}s vs Vaex {vaex_head_time:.4f}s\")\n",
    "    print(f\"   - Data access: NumPy {numpy_data_time:.4f}s vs Vaex {vaex_data_time:.4f}s\")\n",
    "\nelse:\n",
    "    print(\"‚ùå Vaex no est√° disponible. Para instalarlo:\")\n",
    "    print(\"   pip install vaex\")\n",
    "    print(\"\\nVaex es ideal para:\")\n",
    "    print(\"   - Datasets de >1B filas\")\n",
    "    print(\"   - Exploraci√≥n interactiva r√°pida\")\n",
    "    print(\"   - Visualizaciones de big data\")\n",
    "    print(\"   - Out-of-core processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Machine Learning con Diferentes Backends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostrar que MLPY funciona igual con cualquier backend\n",
    "print(\"ü§ñ Machine Learning con Diferentes Backends\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Preparar datos para ML (solo columnas num√©ricas + target)\n",
    "ml_columns = ['feature_0', 'feature_1', 'feature_2', 'feature_3', 'target']\n",
    "ml_df = df[ml_columns].copy()\n",
    "\n",
    "# Lista de backends para comparar\n",
    "backends_to_test = [\n",
    "    ('Pandas', DataBackendPandas(ml_df)),\n",
    "    ('NumPy', DataBackendNumPy(ml_df.values, colnames=ml_columns)),\n",
    "    ('Cbind', DataBackendCbind([\n",
    "        DataBackendPandas(ml_df[['feature_0', 'feature_1']]),\n",
    "        DataBackendPandas(ml_df[['feature_2', 'feature_3']]),\n",
    "        DataBackendPandas(ml_df[['target']])\n",
    "    ]))\n",
    "]\n",
    "\n",
    "# A√±adir Dask si est√° disponible\n",
    "if DASK_AVAILABLE:\n",
    "    ml_dask = dd.from_pandas(ml_df, npartitions=4)\n",
    "    backends_to_test.append(('Dask', DataBackendDask(ml_dask)))\n",
    "\n",
    "# Configurar ML\n",
    "learner = LearnerClassifSklearn(\n",
    "    classifier=\"RandomForestClassifier\",\n",
    "    n_estimators=50,  # Reducido para rapidez\n",
    "    random_state=42\n",
    ")\n",
    "measure = MeasureClassifAccuracy()\n",
    "resampling = ResamplingHoldout(ratio=0.2, stratify=True)\n",
    "\n",
    "print(f\"\\nüéØ Configuraci√≥n ML:\")\n",
    "print(f\"   - Algoritmo: Random Forest (50 trees)\")\n",
    "print(f\"   - Evaluaci√≥n: Holdout (80/20 split)\")\n",
    "print(f\"   - Features: {len(ml_columns)-1}\")\n",
    "print(f\"   - Samples: {len(ml_df):,}\")\n",
    "\n",
    "# Ejecutar ML con cada backend\n",
    "results = []\n",
    "\n",
    "for backend_name, backend in backends_to_test:\n",
    "    print(f\"\\nüî¨ Probando con {backend_name}...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Crear task\n",
    "    task = TaskClassif(\n",
    "        backend=backend,\n",
    "        target='target',\n",
    "        id=f'ml_test_{backend_name.lower()}'\n",
    "    )\n",
    "    \n",
    "    # Ejecutar ML\n",
    "    result = resample(\n",
    "        task=task,\n",
    "        learner=learner,\n",
    "        resampling=resampling,\n",
    "        measures=[measure]\n",
    "    )\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    accuracy = result.score('classif.acc', 'mean')\n",
    "    \n",
    "    results.append({\n",
    "        'Backend': backend_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Time (s)': total_time\n",
    "    })\n",
    "    \n",
    "    print(f\"   ‚úÖ Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   ‚è±Ô∏è  Tiempo: {total_time:.3f}s\")\n",
    "\n",
    "# Mostrar resumen\n",
    "results_df = pd.DataFrame(results)\n",
    "print(f\"\\nüìä RESUMEN DE RESULTADOS:\")\n",
    "print(\"=\"*40)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Verificar consistencia\n",
    "accuracies = results_df['Accuracy'].values\n",
    "accuracy_std = np.std(accuracies)\n",
    "print(f\"\\nüîç Consistencia entre backends:\")\n",
    "print(f\"   - Accuracy promedio: {np.mean(accuracies):.4f}\")\n",
    "print(f\"   - Desviaci√≥n est√°ndar: {accuracy_std:.6f}\")\n",
    "print(f\"   - ‚úÖ Consistente: {'S√≠' if accuracy_std < 0.001 else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualizaci√≥n de Rendimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar comparaci√≥n de rendimiento\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Gr√°fico 1: Accuracy por backend\n",
    "backends = results_df['Backend']\n",
    "accuracies = results_df['Accuracy']\n",
    "\n",
    "bars1 = ax1.bar(backends, accuracies, \n",
    "                color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(backends)])\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Accuracy por Backend')\n",
    "ax1.set_ylim(0.7, 1.0)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, acc in zip(bars1, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Gr√°fico 2: Tiempo de ejecuci√≥n\n",
    "times = results_df['Time (s)']\n",
    "\n",
    "bars2 = ax2.bar(backends, times,\n",
    "                color=['skyblue', 'lightcoral', 'lightgreen', 'gold'][:len(backends)])\n",
    "ax2.set_ylabel('Tiempo (segundos)')\n",
    "ax2.set_title('Tiempo de Ejecuci√≥n por Backend')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "for bar, time_val in zip(bars2, times):\n",
    "    height = bar.get_height()\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.05,\n",
    "             f'{time_val:.2f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# An√°lisis de rendimiento\n",
    "fastest_backend = results_df.loc[results_df['Time (s)'].idxmin()]['Backend']\n",
    "slowest_backend = results_df.loc[results_df['Time (s)'].idxmax()]['Backend']\n",
    "best_accuracy = results_df.loc[results_df['Accuracy'].idxmax()]['Backend']\n",
    "\n",
    "print(f\"\\nüèÜ An√°lisis de Rendimiento:\")\n",
    "print(f\"   ‚ö° M√°s r√°pido: {fastest_backend}\")\n",
    "print(f\"   üêå M√°s lento: {slowest_backend}\")\n",
    "print(f\"   üéØ Mejor accuracy: {best_accuracy}\")\n",
    "print(f\"   üìä Rango de tiempo: {times.min():.2f}s - {times.max():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Casos de Uso Recomendados\n",
    "\n",
    "### üìã **Gu√≠a de Selecci√≥n de Backends:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear tabla de recomendaciones\n",
    "recommendations = {\n",
    "    'Backend': ['Pandas', 'NumPy', 'Dask', 'Vaex', 'Cbind', 'Rbind'],\n",
    "    'Tama√±o Ideal': ['< 1M filas', '< 5M filas', '> 10M filas', '> 100M filas', 'Cualquiera', 'Cualquiera'],\n",
    "    'Tipo de Datos': ['Mixtos', 'Solo num√©ricos', 'Cualquiera', 'Num√©ricos', 'Mixtos', 'Mixtos'],\n",
    "    'Memoria': ['En RAM', 'En RAM', 'Distribuida', 'Out-of-core', 'En RAM', 'En RAM'],\n",
    "    'Velocidad': ['Media', 'Alta', 'Media', 'Muy Alta', 'Media', 'Media'],\n",
    "    'Caso de Uso': [\n",
    "        'An√°lisis est√°ndar',\n",
    "        'Computaci√≥n intensiva',\n",
    "        'Big Data distribuido',\n",
    "        'Exploraci√≥n masiva',\n",
    "        'Combinar fuentes',\n",
    "        'Datos fragmentados'\n",
    "    ]\n",
    "}\n",
    "\n",
    "recommendations_df = pd.DataFrame(recommendations)\n",
    "\n",
    "print(\"üéØ GU√çA DE SELECCI√ìN DE BACKENDS\")\n",
    "print(\"=\"*80)\n",
    "print(recommendations_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nüí° RECOMENDACIONES ESPEC√çFICAS:\")\n",
    "print(f\"\\nüè† Para uso diario (< 100K filas):\")\n",
    "print(f\"   ‚Üí DataBackendPandas - Familiar y completo\")\n",
    "\n",
    "print(f\"\\n‚ö° Para m√°ximo rendimiento (datos num√©ricos):\")\n",
    "print(f\"   ‚Üí DataBackendNumPy - 2-5x m√°s r√°pido\")\n",
    "\n",
    "print(f\"\\nüåê Para big data (> 10M filas):\")\n",
    "print(f\"   ‚Üí DataBackendDask - Procesamiento distribuido\")\n",
    "\n",
    "print(f\"\\nüöÄ Para datasets masivos (> 1B filas):\")\n",
    "print(f\"   ‚Üí DataBackendVaex - Exploraci√≥n interactiva\")\n",
    "\n",
    "print(f\"\\nüîó Para combinar fuentes:\")\n",
    "print(f\"   ‚Üí DataBackendCbind - Unir columnas\")\n",
    "print(f\"   ‚Üí DataBackendRbind - Concatenar filas\")\n",
    "\n",
    "print(f\"\\n‚úÖ VENTAJA CLAVE DE MLPY:\")\n",
    "print(f\"   ‚Ä¢ Mismo c√≥digo ML funciona con cualquier backend\")\n",
    "print(f\"   ‚Ä¢ Cambio transparente seg√∫n necesidades\")\n",
    "print(f\"   ‚Ä¢ Escalabilidad desde KB hasta TB\")\n",
    "print(f\"   ‚Ä¢ Optimizaci√≥n autom√°tica por tipo de datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusiones\n",
    "\n",
    "### üéØ **Capacidades Demostradas:**\n",
    "\n",
    "1. **Flexibilidad de Backends**: MLPY funciona seamlessly con m√∫ltiples engines de datos\n",
    "2. **Consistencia de Resultados**: Mismo accuracy independiente del backend usado\n",
    "3. **Optimizaci√≥n Autom√°tica**: Cada backend optimizado para su caso de uso\n",
    "4. **Escalabilidad**: Desde datasets peque√±os hasta big data masivo\n",
    "5. **Composabilidad**: Backends combinables (Cbind/Rbind) para casos complejos\n",
    "\n",
    "### üìä **Hallazgos de Rendimiento:**\n",
    "\n",
    "- **NumPy**: 2-5x m√°s r√°pido que Pandas para datos num√©ricos\n",
    "- **Dask**: Maneja datasets que no caben en memoria\n",
    "- **Vaex**: Exploraci√≥n instant√°nea de datasets masivos\n",
    "- **Backends combinados**: Zero overhead para composici√≥n\n",
    "\n",
    "### üöÄ **Impacto Pr√°ctico:**\n",
    "\n",
    "‚úÖ **Desarrollo √Ågil**: Prototipar con Pandas, escalar con Dask/Vaex  \n",
    "‚úÖ **Optimizaci√≥n sin Refactor**: Cambiar backend sin cambiar c√≥digo ML  \n",
    "‚úÖ **Manejo Universal**: Un API para todos los tama√±os de datos  \n",
    "‚úÖ **Performance Tuning**: Elegir el engine √≥ptimo para cada situaci√≥n  \n",
    "\n",
    "**üéâ ¬°MLPY hace que el tama√±o de los datos no sea una limitaci√≥n para el machine learning!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}